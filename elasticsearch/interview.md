## 1、Elasticsearch 的倒排索引是什么

倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。
倒排索引的底层实现是基于：FST (Finite State Transducer) 数据结构，FST 有两个优点：
1. 空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；
2. 查询速度快。O(len(str)) 的查询时间复杂度。

## 2、Elasticsearch 在部署时，对 linux 的设置有哪些优化方法

1. 关闭缓存 swap；
2. 堆内存设置为：Min (节点内存/2，32GB)；
3. 设置最大文件句柄数；
4. 线程池+队列大小根据业务需求做调整；
5. 磁盘存储 RAID 方式——存储有条件使用 RAID10，增加单节点性能以及避免单节点存储故障。

## 3、详细描述下 Elasticsearch 索引文档的过程

索引文档：文档写入 ES，创建索引的过程。
文档写入包括：单文档写入和批量 bulk 写入。

1. 客户写集群某节点写入数据，发送请求。(如果没有指定路由/协调节点，请求的节点扮演协调节点的角色)；
2. 节点 1 接收到请求后，使用文档 _id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上；
3. 节点 3 在主分片上执行写的操作，如果成功，则将请求并行转发到节点 1 和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点 (节点 1) 报告成功，节点 1 向请求客户端报告写入成功。

## 4、在并发情况下，Elasticsearch 如何保证读写一致性

1. 可以通过版本号使用乐观锁并发控制，以确保旧版本不会被新版本覆盖，由应用层来处理具体的冲突；
2. 对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大部分分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导入写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建；
3. 对于读操作，可以设置 replication 为 sync (默认)，这使得操作在主分片和副本分片都完成后才会返回。如果设置 replication 为 async，也可以通过设置搜索请求参数 _preference 为 primary 来查询主分片，确保文档是最新版本。

## 5、在 Elasticsearch 集群中添加或创建索引的过程

要添加新索引，应使用创建索引 API 选项。创建索引所需的参数是索引的配置 Settings，索引中的字段 Mapping 以及索引别名 Alias。也可以通过模板 Template 来创建。

## 6、安装 Elasticsearch 需要依赖什么组件吗

ES 早期版本需要 SDK，在 7.x 版本后已经集成了 SDK，已无需第三方依赖。

## 7、如何使用 Elastic Reporting

Reporting API 有助于将检索结果生成 pdf 格式，图像 png 格式以及电子表格 csv 格式的数据，并可根据需要进行共享或保存。（收费功能）

## 8、Elasticsearch 是如何实现 master 选举的

前置前提：
1. 只有候选主节点 (master: true) 的节点才能成为主节点；
2. 最小主节点数 (min_master_nodes) 的目的是防止脑裂。

第一步：确认候选主节点数达标，elasticsearch.yml 设置的值 discovery.zen.minimum_master_nodes；
第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回；若两节点都为候选主节点，则 id 小的值会成为主节点。这里的 id 为 string 类型。

## 9、Elasticsearch 更新和删除文档的过程

1. 更新和删除也是写操作。Elasticsearch 中的文档是不可变的，因此不能被删除或改动；
2. 磁盘上的每一段都是有一个相应的 .del 文件。当删除请求发送后，是在 .del 文件中被标记为删除。该文件依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在 .del 文件中被标记为删除的文档不会被写入新段；
3. 当新的文档被创建时，会为该文档指定一个版本号，当执行更新时，旧版本的文档在 .del 文件中被标记为删除。新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但会在结果中被过滤掉。

## 10、列出 Elasticsearch 各种类型的分析器

Elasticsearch Analyzer 的类型为内置分析器和自定义分析器。

1. Standard Analyzer：标准分析器 (默认)，基于 Unicode 文本分割算法；
2. Whitespace Analyzer：基于空格字符切词；
3. Stop Analyzer：在 Simple Analyzer 的基础上，移除停用词；
4. Keyword Analyzer：不切词，将输入的整个串一起返回；
5. 自定义分析器：
   1. "char_filter":{},——对应字符过滤部分； 
   2. "tokenizer":{},——对应文本切分为分词部分； 
   3. "filter":{},——对应分词后再过滤部分； 
   4. "analyzer":{}——对应分词器组成部分，其中会包含：1. 2. 3
